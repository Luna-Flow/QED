#set page(paper: "a4", margin: (x: 2.4cm, y: 2.2cm))
#set par(justify: true, leading: 0.62em)
#set text(size: 10.5pt)

#let info-color = rgb("#1d4ed8")
#let warning-color = rgb("#ca8a04")
#let critical-color = rgb("#b91c1c")

#let level-color(level) = {
  if level == "info" {
    info-color
  } else if level == "warning" {
    warning-color
  } else {
    critical-color
  }
}

#let keyblock(level, title, body) = {
  let color = level-color(level)
  rect(
    inset: 10pt,
    radius: 5pt,
    stroke: 0.5pt + color,
    fill: color.lighten(86%),
    [
      #text(fill: color)[#title]
      #v(0.35em)
      #body
    ],
  )
}

#align(center)[
  #text(size: 16pt, weight: "bold")[A Formal Mathematical Specification for QED]
  #linebreak()
  #text(size: 11pt)[A Minimal LCF-Style HOL Kernel in MoonBit]
]

#v(1.2em)

= Abstract

QED is an interactive theorem prover in MoonBit, designed around an LCF-style trusted kernel and a higher-order logic foundation. This document gives a formal, implementation-aware specification of its core theory. The presentation starts from first principles: signatures, type formation, term formation, typing judgments, substitution laws, theorem objects, and primitive inference rules. The objective is to make the trust model and proof discipline explicit enough that a reader can understand the logic of the system without reading source code.

#v(0.8em)
#keyblock("info", [Reading Guide], [
  The document is intentionally ordered from basic theory to derived engineering consequences. A reader should be able to stop after the foundational sections and still obtain a coherent understanding of the QED kernel.
])

#set heading(numbering: "1.")

= Motivation and Design Goal

The central engineering goal of QED is to separate trusted reasoning from untrusted proof search. In an LCF architecture, theorem creation is restricted to a very small set of primitive kernel operations. Everything else, including automation and tactics, is merely a theorem-producing program that calls those primitives.

This architecture has two consequences:

1. Correctness is reduced to the soundness of a small kernel.
2. Rich user tooling can evolve without expanding the trusted code base.

The mathematical role of this document is to state the object language and inference rules precisely enough to support both consequences.

= Foundational Theory

== Signatures and Symbols

Let $Sigma_t$ be a type-constructor signature. Each constructor $k in Sigma_t$ has a natural-number arity $a(k) in NN$.

Let $Sigma_c$ be a term-constant signature. Each constant $c in Sigma_c$ is assigned a simple type.

In the current QED kernel, $Sigma_c$ is managed by a scoped signature stack:

- each scope stores a finite partial map from constant names to types;
- lookup proceeds from the innermost scope to the outermost scope;
- same-name insertion is rejected inside one scope but allowed in a nested scope (shadowing).

QED reserves two distinguished type constructors:

- $"bool"$ with arity $0$.
- $"fun"$ with arity $2$.

These are sufficient to define simply typed lambda terms with boolean propositions.

== Type Grammar

Types are generated by the grammar
$
  tau ::= alpha | k(tau_1, ..., tau_n)
$
where $alpha$ ranges over type variables and $k in Sigma_t$ with $a(k) = n$.

In implementation terms, QED uses:

- `TyVal(name)` for type variables.
- `TyApp(tycon, args)` for constructor application.

This representation is syntax-directed and supports recursive operations such as type substitution and constructor decomposition.

== Term Grammar

Terms are generated by
$
  t ::= x : tau | c : tau | t_1 t_2 | λ (x : tau). t
$
where $x$ is a variable symbol and $c$ is a constant symbol.

In implementation terms, QED uses:

- `Var(name, tau)`
- `Const(name, tau)`
- `Comb(f, x)`
- `Abs(x, t)`

The abstraction constructor is intended to bind occurrences of the variable component of `x` in `t`.

#keyblock("warning", [Binding and Capture], [
  Any substitution algorithm used by the kernel must be capture-avoiding. This is not a convenience detail: it is a semantic requirement for soundness.
])

= Typing Judgments

The type system is syntax-directed and is presented as a judgment of the form
$
  Gamma tack.r t : tau
$
where $Gamma$ is a typing context for free variables.

The intended rules are the standard STLC-style rules over simple types:

$
  (" "(x : tau) in Gamma" ") / (Gamma tack.r x : tau)
$

$
  (" "Sigma_c(c) = tau" ") / (Gamma tack.r c : tau)
$

$
  (" "Gamma tack.r f : "fun"(tau_1, tau_2) and Gamma tack.r x : tau_1" ")
  /
  (Gamma tack.r f x : tau_2)
$

$
  (Gamma, x : tau_1 tack.r t : tau_2)
  /
  (" "Gamma tack.r λ (x : tau_1). t : "fun"(tau_1, tau_2)" ")
$

The implementation-level helper `type_of(t)` is expected to agree with this judgment for well-formed terms.

= Substitution and Alpha-Equivalence

== Type Substitution

Type substitution is a mapping `theta : type_variable -> hol_type` that extends structurally to types and terms.

For a type variable $alpha$,

- $theta(alpha)$ if defined,
- otherwise $alpha$.

For type application $k(tau_1, ..., tau_n)$,

- apply $theta$ recursively to each argument.

== Term Substitution

Term substitution is a finite map from variables to terms. It must satisfy two constraints:

1. Type preservation: replacement terms match the declared type of replaced variables.
2. Capture avoidance: bound variables may require renaming before substitution under abstraction.

== Alpha-Equivalence

Alpha-equivalence, written $t_1 equiv_alpha t_2$, identifies terms up to systematic renaming of bound variables. It is required by multiple kernel operations, including theorem transitivity-style checks where structural syntax should not distinguish alpha-variants.

= Boundary Conversion and Scoped Shadowing

QED currently uses a two-layer boundary:

- external-facing terms/theorems are in named syntax;
- kernel rule cores execute on De Bruijn syntax.

Boundary conversion functions are used with the following Haskell-style signatures:

$
  #math.italic("Term") _arrow.b :: #math.italic("Term") arrow.r #math.italic("DbTerm")?
$
$
  #math.italic("Term") _arrow.t :: #math.italic("DbTerm") arrow.r #math.italic("Term")?
$
$
  #math.italic("Thm") _arrow.b :: #math.italic("Thm") arrow.r #math.italic("DbSequent")?
$
$
  #math.italic("Thm") _arrow.t :: #math.italic("DbSequent") arrow.r #math.italic("Thm")?
$

Here $#math.italic("Term") _arrow.b$ lowers named terms to De Bruijn terms, and $#math.italic("Term") _arrow.t$ reconstructs named terms from De Bruijn terms. Likewise, $#math.italic("Thm") _arrow.b$ lowers named sequents to De Bruijn sequents, and $#math.italic("Thm") _arrow.t$ lifts De Bruijn sequents back to named boundary objects. All four conversions are partial and may fail at the boundary.

For theorem objects:
$
  #math.italic("Thm") _arrow.b " " (A_p tack.r p) = A_d tack.r p_d
$
and
$
  #math.italic("Thm") _arrow.t " " (A_d tack.r p_d) = A_p' tack.r p'
$
defined pointwise by $#math.italic("Term") _arrow.b$ and $#math.italic("Term") _arrow.t$ on assumptions and conclusion.

== Boundary Conversion Properties

The kernel implementation relies on the following invariants.

Lemma (Alpha-Invariant Lowering):
$
  (" "t_1 equiv_alpha t_2" ")
  /
  (#math.italic("Term") _arrow.b " " t_1 = #math.italic("Term") _arrow.b " " t_2)
$

Lemma (Round-Trip Stability up to Alpha):
$
  (" "#math.italic("Term") _arrow.b " " t = d and #math.italic("Term") _arrow.t " " d = t'" ")
  /
  (t' equiv_alpha t)
$

Lemma (Rule Lifting Safety):
if a primitive core rule $R_d$ satisfies
$
  R_d : "DbSequent"^n -> "DbSequent"?
$
then the lifted rule
$
  R " " x = #math.italic("Thm") _arrow.t " " (R_d " " (#math.italic("Thm") _arrow.b " " x))
$
preserves assumption/conclusion structure modulo alpha-equivalence whenever conversions succeed.

#keyblock("info", [Boundary Discipline], [
  Primitive rules are implemented on `DbSequent`. Named `Thm` values are boundary objects. Conversion failure is treated as boundary failure, not as logical success.
])

== Scoped Shadowing Properties

Let a signature state be a stack
$
  S = [S_0, S_1, ..., S_n]
$
where $S_n$ is the innermost scope.

Lookup is defined by:
$
  L(S, c) = S_j(c)
$
where $j$ is the greatest index such that $c in "dom"(S_j)$.

Insertion in current scope:
$
  A(S, c : tau)
$
is allowed iff $c ∉ "dom"(S_n)$.

From these definitions:

Proposition (Shadowing Determinism):
if $c$ is defined in innermost scope $S_n$, then $L(S, c) = S_n(c)$.

Proposition (Outer Restoration by Pop):
if $S' = P(S)$, $A(S', c : tau_1) = S''$, and $Q(S'') = S$, then $L(S, c) = L(Q(S''), c)$.

Proposition (Scope-Local Uniqueness):
if $c$ is already defined in innermost scope $S_n$, then $A(S, c : tau)$ fails.

These properties specify the intended behavior of `sig_push_scope`, `sig_pop_scope`, `sig_lookup_const`, and scoped insertion APIs.

= Theorem Object and Trust Boundary

A theorem is represented mathematically as a sequent
$
  Gamma_p tack.r p
$
with $p$ a boolean term and $Gamma_p$ a finite set of boolean assumptions.

The trusted boundary condition is:

- external modules cannot directly construct theorem values,
- theorem values are produced only by primitive kernel inference functions.

This boundary is the core LCF invariant.

#keyblock("critical", [Kernel Integrity Condition], [
  If external code can fabricate theorem values, the entire soundness argument collapses, regardless of how correct individual inference rules appear.
])

= Primitive Inference Rules

QED follows a HOL Light style primitive interface:

`REFL`, `ASSUME`, `TRANS`, `MK_COMB`, `ABS`, `BETA`, `EQ_MP`, `DEDUCT_ANTISYM_RULE`, `INST_TYPE`, `INST`.

Each primitive rule must be specified by:

1. Input theorem and term constraints.
2. Side conditions (typing, freeness, alpha-matching, etc.).
3. Output sequent.
4. Failure condition classification.

For example, selected rules can be presented in antecedent style:

- `REFL`: for any term $t$, conclude $tack.r t = t$.
- `ASSUME`: for any boolean proposition $p$, conclude $p tack.r p$.

$
  (" "A_p tack.r s = t and B_p tack.r t = u" ")
  /
  (A_p union B_p tack.r s = u)
$

$
  (" "A_p tack.r p = q and B_p tack.r p" ")
  /
  (A_p union B_p tack.r q)
$

Detailed formal side conditions are maintained in parallel with implementation updates.

== Rule Schema: `REFL`

Input:

- a well-formed term $t$.

Output:

- theorem $tack.r t = t$.

Side conditions:

1. $t$ must be typable.
2. equality constructor must be formed at the type of $t$.

Failure clauses:

1. malformed term input;
2. type construction failure in equality formation.

Antecedent form:
$
  (" "Gamma tack.r t : T" ") / (tack.r t = t)
$

== Rule Schema: `ASSUME`

Input:

- a proposition term $p$.

Output:

- theorem $p tack.r p$.

Side conditions:

1. $p$ must have type $"bool"$.
2. assumption set representation must admit $p$.

Failure clauses:

1. non-boolean proposition;
2. invalid assumption-set insertion.

Antecedent form:
$
  (" "Gamma_p tack.r p : "bool"" ") / (p tack.r p)
$

== Rule Schema: `TRANS`

Input:

- theorem $A_p tack.r s = t$;
- theorem $B_p tack.r t = u$.

Output:

- theorem $A_p union B_p tack.r s = u$.

Side conditions:

1. both conclusions must be equalities;
2. the middle terms must match up to alpha-equivalence and type consistency.

Failure clauses:

1. non-equality conclusion in either premise theorem;
2. middle-term mismatch;
3. type inconsistency in chained equality.

Antecedent form:
$
  (" "A_p tack.r s = t and B_p tack.r t = u" ")
  /
  (A_p union B_p tack.r s = u)
$

== Rule Schema: `MK_COMB`

Input:

- theorem $A_p tack.r f = g$;
- theorem $B_p tack.r x = y$.

Output:

- theorem $A_p union B_p tack.r f x = g y$.

Side conditions:

1. both premise conclusions must be equalities;
2. $f$ and $g$ must have function type with argument type matching $x$ and $y$;
3. codomain types of $f$ and $g$ must coincide.

Failure clauses:

1. non-equality premise theorem;
2. function-domain mismatch for application;
3. codomain inconsistency across the two function sides.

Antecedent form:
$
  (" "A_p tack.r f = g and B_p tack.r x = y" ")
  /
  (A_p union B_p tack.r f x = g y)
$

== Rule Schema: `ABS`

Input:

- variable term $x$;
- theorem $A_p tack.r s = t$.

Output:

- theorem $A_p tack.r λ (x : tau). s = λ (x : tau). t$.

Side conditions:

1. $x$ must be a variable term;
2. premise conclusion must be an equality;
3. $x$ must not occur free in assumptions $A_p$.

Failure clauses:

1. non-variable abstraction binder;
2. non-equality premise theorem;
3. free-variable violation in assumption set.

Antecedent form:
$
  (A_p tack.r s = t)
  /
  (" "A_p tack.r λ (x : tau). s = λ (x : tau). t" ")
$

== Rule Schema: `BETA`

Input:

- a typed beta-redex term of shape $(λ (x : tau). s) u$.

Output:

- theorem $tack.r ((λ (x : tau). s) u) = s[u/x]$.

Side conditions:

1. the redex must be well-typed;
2. substitution is capture-avoiding.

Failure clauses:

1. input is not a beta-redex of the required shape;
2. type inconsistency in redex construction;
3. boundary reconstruction failure.

Antecedent form:
$
  (" "t = (λ (x : tau). s) u and "welltyped(t)"" ")
  /
  (tack.r t = s[u/x])
$

== Rule Schema: `EQ_MP`

Input:

- theorem $A_p tack.r p = q$;
- theorem $B_p tack.r p$.

Output:

- theorem $A_p union B_p tack.r q$.

Side conditions:

1. first premise must conclude an equality proposition;
2. left side of equality must match the second premise conclusion up to alpha-equivalence;
3. all involved terms must be boolean propositions.

Failure clauses:

1. first premise is not an equality theorem;
2. proposition mismatch between equality lhs and premise theorem;
3. non-boolean proposition in premises.

Antecedent form:
$
  (" "A_p tack.r p = q and B_p tack.r p" ")
  /
  (A_p union B_p tack.r q)
$

== Rule Schema: `DEDUCT_ANTISYM_RULE`

Input:

- theorem $A_p tack.r p$;
- theorem $B_p tack.r q$.

Output:

- theorem $(A_p - {q}) union (B_p - {p}) tack.r p = q$.

Side conditions:

1. both premises must conclude propositions;
2. subtraction from assumption sets must be defined by alpha-aware proposition equality;
3. resulting assumption set must remain finite.

Failure clauses:

1. malformed assumption-set subtraction;
2. proposition mismatch in set-removal targets;
3. non-propositional premise conclusion.

Antecedent form:
$
  (" "A_p tack.r p and B_p tack.r q" ")
  /
  ((A_p - {q}) union (B_p - {p}) tack.r p = q)
$

== Rule Schema: `INST_TYPE`

Input:

- type substitution $theta$;
- theorem $A_p tack.r p$.

Output:

- theorem $theta(A_p) tack.r theta(p)$.

Side conditions:

1. substitution domain must contain only type variables;
2. substitution application must preserve term well-typedness;
3. theorem structure must be preserved under parallel type substitution.

Failure clauses:

1. invalid substitution mapping;
2. typing failure after substitution;
3. malformed theorem structure under substitution.

Antecedent form:
$
  (" "A_p tack.r p and "valid(theta)"" ")
  /
  (theta(A_p) tack.r theta(p))
$

== Rule Schema: `INST`

Input:

- term substitution $sigma$;
- theorem $A_p tack.r p$.

Output:

- theorem $sigma(A_p) tack.r sigma(p)$.

Side conditions:

1. substitution domain must contain only variable terms;
2. each mapped term in $sigma$ must have the same type as its source variable;
3. substitution must be capture-avoiding and applied in parallel.

Failure clauses:

1. non-variable key in substitution map;
2. type mismatch in substitution pair;
3. variable capture or malformed substitution application.

Antecedent form:
$
  (" "A_p tack.r p and "valid(sigma)"" ")
  /
  (sigma(A_p) tack.r sigma(p))
$

= Soundness Strategy

The project-level soundness story is divided into three obligations.

1. Rule-level preservation: every primitive rule preserves semantic validity.
2. Interface safety: only primitive rules can introduce theorem values.
3. Derivation closure: any finite derivation tree built from primitive rules is sound.

This decomposition is practical: it aligns the formal argument with module boundaries and test responsibilities.

= Engineering Correspondence

The formal clauses above map to implementation modules as follows.

- `src/kernel/types.mbt`: type constructors, decomposers, predicates, and type-level operators.
- `src/kernel/terms.mbt`: term constructors, decomposers, typing helper, and term-level operators.
- `src/kernel/thm.mbt`: theorem abstraction and primitive rule implementation.
- `src/kernel/sig.mbt`: scoped signature stack, constant registration, and definitional signature operations.

A development task is complete only when the mathematical clause and its implementation clause are both updated.

== Rule-to-Implementation Mapping (Current)

- `REFL` -> `src/kernel/thm.mbt` (implemented; De Bruijn core + boundary lift).
- `ASSUME` -> `src/kernel/thm.mbt` (implemented; De Bruijn core + boundary lift).
- `TRANS` -> `src/kernel/thm.mbt` (implemented; De Bruijn core + boundary lift).
- `MK_COMB` -> `src/kernel/thm.mbt` (implemented; De Bruijn core + boundary lift).
- `ABS` -> `src/kernel/thm.mbt` (implemented; De Bruijn core + boundary lift).
- `BETA` -> `src/kernel/thm.mbt` (implemented; De Bruijn beta core + boundary lift).
- `EQ_MP` -> `src/kernel/thm.mbt` (implemented; De Bruijn core + boundary lift).
- `DEDUCT_ANTISYM_RULE` -> `src/kernel/thm.mbt` (implemented; De Bruijn core + boundary lift).
- `INST_TYPE` -> `src/kernel/thm.mbt` (implemented; De Bruijn substitution core + boundary lift).
- `INST` -> `src/kernel/thm.mbt` (implemented; De Bruijn substitution core + boundary lift).

== Design Delta vs HOL Light

QED and HOL Light share the LCF principle and primitive-rule trust model, but QED currently differs in two engineering choices:

1. Rule execution layer:
  HOL Light executes directly over named terms; QED executes rule cores over De Bruijn objects and lifts results to named boundaries.
2. Constant signature policy:
  HOL Light uses globally unique constant naming; QED currently uses scoped signature stacks with explicit shadowing.

These deltas are intentional and must be read as implementation-level policy choices, not changes to the object-logic proposition/equality calculus.

#keyblock("warning", [Error Semantics Status], [
  Full kernel-wide structured error propagation is still in migration. Current interfaces mix option-style failures with typed error returns in selected modules.
])

= Documentation Roadmap

This document is a living formal artifact. The target final version includes:

1. full rule-by-rule formalization of all primitive inference rules,
2. explicit substitution lemmas and alpha-equivalence lemmas,
3. a consistency assumptions section,
4. an interface theorem connecting kernel and tactic layers,
5. a revision log linking formal clauses to commit history.

#keyblock("info", [Current Status], [
  The current manuscript is a foundational draft emphasizing theory and system principles. Subsequent revisions will increase proof detail and rule-level formal completeness.
])
